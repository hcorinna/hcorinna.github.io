<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Corinna Hertweck | Publications</title>
    <meta name="author" content="Corinna  Hertweck" />
    <meta name="description" content="Publications and preprints." />
    <meta name="keywords" content="algorithmic-fairness, fair-ML" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⛰️</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://hcorinna.github.io/publications/">
    
    <!-- Dark Mode -->
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://hcorinna.github.io/">Corinna Hertweck</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">CV</a>
              </li>
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Publications</h1>
            <p class="post-description">Publications and preprints.</p>
          </header>

          <article>
            <!-- _pages/publications.md -->
<div class="publications">

<p>
If you're doing research in a similar area, you might find this list of relevant conferences and deadlines helpful: <a href="https://hcorinna.github.io/fair-deadlines/?sub=AIE,CSS,MD,ML,DM,CV" target="_blank">https://hcorinna.github.io/fair-deadlines/</a>
</p>

<h1>Conferences</h1>

  <h2 class="year">2024</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">AAAI/ACM</abbr></div>

        <!-- Entry bib key -->
        <div id="hertweck2024distributive" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">What’s Distributive Justice Got to Do with It? Rethinking Algorithmic Fairness from a Perspective of Approximate Justice</div>
          <!-- Author -->
          <div class="author">
                  <em>Hertweck, Corinna</em>, Heitz, Christoph, and Loi, Michele
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</em> (2024)
          </div>

          
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://ojs.aaai.org/index.php/AIES/article/view/31661" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
            <a href="http://arxiv.org/abs/2407.12488" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">IEEE</abbr></div>

        <!-- Entry bib key -->
        <div id="hertweck2024group" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Group Fairness Refocused: Assessing the Social Impact of ML Systems</div>
          <!-- Author -->
          <div class="author">
                  <em>Hertweck, Corinna</em>, Loi, Michele, and Heitz, Christoph
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2024 11th Swiss Conference on Data Science (SDS)</em> (2024)
          </div>

          
            <div class="award">Honorable Mention - Best Paper</div>
          
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Fairness as a property of a prediction-based decision system is a question of its impact on the lives of affected people, which is only partially captured by standard fairness metrics. In this paper, we present a formal framework for the impact assessment of prediction-based decision systems based on the paradigm of group fairness. We generalize the equality requirements of standard fairness criteria to the concept of equality of expected impact, and we show that standard fairness criteria can be interpreted as special cases of this generalization. Furthermore, we provide a systematic and practical method for determining the necessary utility functions for modeling the impact. We conclude with a discussion of possible extensions of our approach.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ACM</abbr></div>

        <!-- Entry bib key -->
        <div id="vigano2022people" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">People Are Not Coins: Morally Distinct Types of Predictions Necessitate Different Fairness Constraints</div>
          <!-- Author -->
          <div class="author">Viganò, Eleonora, 
                  <em>Hertweck, Corinna</em>, Heitz, Christoph, and Loi, Michele
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em> (2022)
          </div>

          
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://doi.org/10.1145/3531146.3534643" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
            <a href="https://ssrn.com/abstract=3857889" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">SSRN</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>A recent paper (Hedden 2021) has argued that most of the group fairness constraints discussed in the machine learning literature are not necessary conditions for the fairness of predictions, and hence that there are no genuine fairness metrics. This is proven by discussing a special case of a fair prediction. In our paper, we show that Hedden’s argument does not hold for the most common kind of predictions used in data science, which are about people and based on data from similar people; we call these “human-group-based practices”. We argue that there is a morally salient distinction between human-group-based practices and those that are based on data of only one person, which we call “human-individual-based practices”. Thus, what may be a necessary condition for the fairness of human-group-based practices may not be a necessary condition for the fairness of human-individual-based practices, on which Hedden’s argument is based. Accordingly, the group fairness metrics discussed in the machine learning literature may still be relevant for most applications of prediction-based decision making.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">AAAI</abbr></div>

        <!-- Entry bib key -->
        <div id="hertweck2021gradual" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Gradual (In)Compatibility of Fairness Criteria</div>
          <!-- Author -->
          <div class="author">
                  <em>Hertweck, Corinna</em>, and Räz, Tim
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 36th AAAI Conference on Artificial Intelligence</em> (2022)
          </div>

          
            <div class="award">Oral Presentation</div>
          
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://doi.org/10.1609/aaai.v36i11.21450" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
            <a href="http://arxiv.org/abs/2109.04399" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://github.com/hcorinna/gradual-compatibility" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="/assets/pdf/aaai_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Impossibility results show that important fairness measures (independence, separation, sufficiency) cannot be satisfied at the same time under reasonable assumptions. This paper explores whether we can satisfy and/or improve these fairness measures simultaneously to a certain degree. We introduce information-theoretic formulations of the fairness measures and define degrees of fairness based on these formulations. The information-theoretic formulations suggest unexplored theoretical relations between the three fairness measures. In the experimental part, we use the information-theoretic expressions as regularizers to obtain fairness-regularized predictors for three standard datasets. Our experiments show that a) fairness regularization directly increases fairness measures, in line with existing work, and b) some fairness regularizations indirectly increase other fairness measures, as suggested by our theoretical findings. This establishes that it is possible to increase the degree to which some fairness measures are satisfied at the same time – some fairness measures are gradually compatible.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">IEEE</abbr></div>

        <!-- Entry bib key -->
        <div id="hertweck2021systematic" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">A Systematic Approach to Group Fairness in Automated Decision Making</div>
          <!-- Author -->
          <div class="author">
                  <em>Hertweck, Corinna</em>, and Heitz, Christoph
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2021 8th Swiss Conference on Data Science (SDS)</em> (2021)
          </div>

          
            <div class="award">Best Paper in Ethical, Legal, and Social Issues (ELSI) of Data Science</div>
          
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://doi.org/10.1109/SDS51136.2021.00008" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
            <a href="http://arxiv.org/abs/2109.04230" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>While the field of algorithmic fairness has brought forth many ways to measure and improve the fairness of machine learning models, these findings are still not widely used in practice. We suspect that one reason for this is that the field of algorithmic fairness came up with a lot of definitions of fairness, which are difficult to navigate. The goal of this paper is to provide data scientists with an accessible introduction to group fairness metrics and to give some insight into the philosophical reasoning for caring about these metrics. We will do this by considering in which sense socio-demographic groups are compared for making a statement on fairness.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ACM</abbr></div>

        <!-- Entry bib key -->
        <div id="hertweck2021moral" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">On the Moral Justification of Statistical Parity</div>
          <!-- Author -->
          <div class="author">
                  <em>Hertweck, Corinna</em>, Heitz, Christoph, and Loi, Michele
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em> (2021)
          </div>

          
            <div class="award">Best Student Paper</div>
          
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://doi.org/10.1145/3442188.3445936" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
            <a href="http://arxiv.org/abs/2011.02079" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="/assets/pdf/facct_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>A crucial but often neglected aspect of algorithmic fairness is the question of how we justify enforcing a certain fairness metric from a moral perspective. When fairness metrics are proposed, they are typically argued for by highlighting their mathematical properties. Rarely are the moral assumptions beneath the metric explained. Our aim in this paper is to consider the moral aspects associated with the statistical fairness criterion of independence (statistical parity). To this end, we consider previous work, which discusses the two worldviews "What You See Is What You Get" (WYSIWYG) and "We’re All Equal" (WAE) and by doing so provides some guidance for clarifying the possible assumptions in the design of algorithms. We present an extension of this work, which centers on morality. The most natural moral extension is that independence needs to be fulfilled if and only if differences in predictive features (e.g. high school grades and standardized test scores are predictive of performance at university) between socio-demographic groups are caused by unjust social disparities or measurement errors. Through two counterexamples, we demonstrate that this extension is not universally true. This means that the question of whether independence should be used or not cannot be satisfactorily answered by only considering the justness of differences in the predictive features.</p>
          </div>
        </div>
      </div>
</li>
</ol>


<h1>Journals</h1>

  <h2 class="year">2024</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">CLSR</abbr></div>

        <!-- Entry bib key -->
        <div id="hoch2021discrimination" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Discrimination for the Sake of Fairness: Fairness by Design and Its Legal Framework</div>
          <!-- Author -->
          <div class="author">Hoch, Holly, 
                  <em>Hertweck, Corinna</em>, Loi, Michele, and Tamò, Aurelia
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Computer Law &amp; Security Review</em> (2024)
          </div>

          
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://doi.org/10.1016/j.clsr.2023.105916" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
            <a href="https://ssrn.com/abstract=https://ssrn.com/abstract=3773766" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">SSRN</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>As algorithms are increasingly enlisted to make critical determinations about human actors, the more frequently we see these algorithms appear in sensational headlines crying foul on discrimination. There is broad consensus among computer scientists working on this issue that such discrimination can be reduced by intentionally collecting and consciously using sensitive information about demographic features like sex, gender, race, religion etc. Companies implementing such algorithms might, however, be wary of allowing algorithms access to such data as they fear legal repercussions, as the promoted standard has been to omit protected attributes, otherwise dubbed “fairness through unawareness”. This paper asks whether such wariness is justified in light of EU data protection and anti-discrimination laws. In order to answer this question, we introduce a specific case and analyze how EU law might apply when an algorithm accesses sensitive information to make fairer predictions. We review whether such measures constitute discrimination, and for who, arriving at different conclusions based on how we define the harm of discrimination and the groups we compare. Finding that several legal claims could arise regarding the use of sensitive information, we ultimately conclude that the proffered fairness measures would be considered a positive (or affirmative) action under EU law. As such, the appropriate use of sensitive information in order to increase the fairness of an algorithm is a positive action, and not per se prohibited by EU law.</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">Frontiers</abbr></div>

        <!-- Entry bib key -->
        <div id="zaman2022methods" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Methods for Uncovering Discourses That Shape the Urban Imaginary in Helsinki’s Smart City</div>
          <!-- Author -->
          <div class="author">Zaman, Sara, and <em>Hertweck, Corinna</em>
                
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Frontiers in Sustainable Cities</em> (2022)
          </div>

          
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://www.frontiersin.org/article/10.3389/frsc.2022.796469" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
            <a href="https://github.com/hcorinna/smartHEL" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In modern urban environments the technologies that are basic to everyday life have become further embedded in that life. Smart cities are one example of the acceleration of technological change in order to engage with urban sustainability challenges, with Artificial Intelligence (AI) tools as one mode of engagement. However, the discourses through which cities engage with smart city growth and management can have long-term consequences for diverse knowledge held within the imaginaries of situated smart urbanism. As the city of Helsinki increasingly focuses on sustainable smart city initiatives, concurrent research suggests that smart urbanism is at a crossroads, where developers must decide how smart cities choose to engage with its residents’ knowledge. This research sets out to ask, how are top-down smart city interventions communicated on Twitter (de)legitimizing diverse knowledge in situated smart urbanism? We draw from Foucaudian theory to identify which discourses are elevated, through statements posted on the social media platform Twitter. By answering this question, our goal in this paper is to examine how Foucault’s methods can be used to highlight unseen assumptions about smart urbanism in Helsinki. Our objective is to identify overarching narratives and potential contested conceptualizations of smart urbanism in Helsinki. With our methods, we contribute a novel angle to surfacing power relations that are becoming evident in the development of AI-governed smart cities.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">JLA</abbr></div>

        <!-- Entry bib key -->
        <div id="hertweck2022designing" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Designing Affirmative Action Policies under Uncertainty</div>
          <!-- Author -->
          <div class="author">
                  <em>Hertweck, Corinna</em>, Castillo, Carlos, and Mathioudakis, Michael
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Journal of Learning Analytics</em> (2022)
          </div>

          
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://learning-analytics.info/index.php/JLA/article/view/7463" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
            <a href="https://github.com/hcorinna/university-admissions" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We study university admissions under a centralized system that uses grades and standardized test scores to match applicants to university programs. In the context of this system, we explore affirmative action policies that seek to increase the acceptance rate of underrepresented groups while still accepting students with high scores. Since there is uncertainty about the score distribution of the students who will apply to each program, it is unclear what policy would have the desired effect on the acceptance rates of different groups. We address this challenge by using a predictive model trained on historical data to help optimize the parameters of such policies. We find that a learned predictive model does significantly better than relying on the ideal parameters for the last year. At the same time, we also find that a large pool of historical data yields similar results as our predictive approach for our data. Due to the more complex nature of the predictive approach, we conclude that a simpler approach should be preferred if enough data is available (e.g., long-standing, traditional university programs), but not for newer programs and other cases in which our predictive strategy can prove helpful.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">Springer</abbr></div>

        <!-- Entry bib key -->
        <div id="wehrli2021bias" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Bias, awareness, and ignorance in deep-learning-based face recognition</div>
          <!-- Author -->
          <div class="author">Wehrli, Samuel, 
                  <em>Hertweck, Corinna</em>, Amirian, Mohammadreza, Glüge, Stefan, and Stadelmann, Thilo
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>AI and Ethics</em> (2021)
          </div>

          
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://doi.org/10.1007/s43681-021-00108-6" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Face Recognition (FR) is increasingly influencing our lives: we use it to unlock our phones; police uses it to identify suspects. Two main concerns are associated with this increase in facial recognition: (1) the fact that these systems are typically less accurate for marginalized groups, which can be described as “bias”, and (2) the increased surveillance through these systems. Our paper is concerned with the first issue. Specifically, we explore an intuitive technique for reducing this bias, namely “blinding” models to sensitive features, such as gender or race, and show why this cannot be equated with reducing bias. Even when not designed for this task, facial recognition models can deduce sensitive features, such as gender or race, from pictures of faces—simply because they are trained to determine the “similarity” of pictures. This means that people with similar skin tones, similar hair length, etc. will be seen as similar by facial recognition models. When confronted with biased decision-making by humans, one approach taken in job application screening is to “blind” the human decision-makers to sensitive attributes such as gender and race by not showing pictures of the applicants. Based on a similar idea, one might think that if facial recognition models were less aware of these sensitive features, the difference in accuracy between groups would decrease. We evaluate this assumption—which has already penetrated into the scientific literature as a valid de-biasing method—by measuring how “aware” models are of sensitive features and correlating this with differences in accuracy. In particular, we blind pre-trained models to make them less aware of sensitive attributes. We find that awareness and accuracy do not positively correlate, i.e., that bias ≠ awareness. In fact, blinding barely affects accuracy in our experiments. The seemingly simple solution of decreasing bias in facial recognition rates by reducing awareness of sensitive features does thus not work in practice: trying to ignore sensitive attributes is not a viable concept for less biased FR.</p>
          </div>
        </div>
      </div>
</li></ol>


<h1>Preprints</h1>
<ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="https://doi.org/10.48550/arxiv.2206.02897" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Distributive Justice as the Foundational Premise of Fair ML: Unification, Extension, and Interpretation of Group Fairness Metrics</div>
          <!-- Author -->
          <div class="author">Baumann, Joachim, 
                  <em>Hertweck, Corinna</em>, Loi, Michele, and Heitz, Christoph
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>arXiv</em> (2022)
          </div>

          
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2206.02897" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="/assets/pdf/distributive_justice_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Group fairness metrics are an established way of assessing the fairness of prediction-based decision-making systems. However, these metrics are still insufficiently linked to philosophical theories, and their moral meaning is often unclear. We propose a general framework for analyzing the fairness of decision systems based on theories of distributive justice, encompassing different established “patterns of justice” that correspond to different normative positions. We show that the most popular group fairness metrics can be interpreted as special cases of our approach. Thus, we provide a unifying and interpretative framework for group fairness metrics that reveals the normative choices associated with each of them and that allows understanding their moral substance. At the same time, we provide an extension of the space of possible fairness metrics beyond the ones currently discussed in the fair ML literature. Our framework also allows overcoming several limitations of group fairness metrics that have been criticized in the literature, most notably (1) that they are parity-based, i.e., that they demand some form of equality between groups, which may sometimes be harmful to marginalized groups, (2) that they only compare decisions across groups, but not the resulting consequences for these groups, and (3) that the full breadth of the distributive justice literature is not sufficiently represented.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="https://doi.org/10.48550/arxiv.2206.02891" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">A Justice-Based Framework for the Analysis of Algorithmic Fairness-Utility Trade-Offs</div>
          <!-- Author -->
          <div class="author">
                  <em>Hertweck, Corinna</em>, Baumann, Joachim, Loi, Michele, Viganò, Eleonora, and Heitz, Christoph
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>arXiv</em> (2022)
          </div>

          
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2206.02891" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In prediction-based decision-making systems, different perspectives can be at odds: The short-term business goals of the decision makers are often in conflict with the decision subjects’ wish to be treated fairly. Balancing these two perspectives is a question of values. We provide a framework to make these value-laden choices clearly visible. For this, we assume that we are given a trained model and want to find decision rules that balance the perspective of the decision maker and of the decision subjects. We provide an approach to formalize both perspectives, i.e., to assess the utility of the decision maker and the fairness towards the decision subjects. In both cases, the idea is to elicit values from decision makers and decision subjects that are then turned into something measurable. For the fairness evaluation, we build on the literature on welfare-based fairness and ask what a fair distribution of utility (or welfare) looks like. In this step, we build on well-known theories of distributive justice. This allows us to derive a fairness score that we then compare to the decision maker’s utility for many different decision rules. This way, we provide an approach for balancing the utility of the decision maker and the fairness towards the decision subjects for a prediction-based decision-making system.</p>
          </div>
        </div>
      </div>
</li>
</ol>

</div>

          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Corinna  Hertweck. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.
<a href="https://hcorinna.github.io/impressum/">Impressum</a>.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

