---
---

@article{https://doi.org/10.48550/arxiv.2206.02897,
  title = {Distributive Justice as the Foundational Premise of Fair ML: Unification, Extension, and Interpretation of Group Fairness Metrics},
  author = {Baumann, Joachim and Hertweck, Corinna and Loi, Michele and Heitz, Christoph},
  journal = {arXiv},
  doi = {10.48550/ARXIV.2206.02897},
  year = {2022},
  arxiv = {2206.02897},
  abstract = {Group fairness metrics are an established way of assessing the fairness of prediction-based decision-making systems. However, these metrics are still insufficiently linked to philosophical theories, and their moral meaning is often unclear. We propose a general framework for analyzing the fairness of decision systems based on theories of distributive justice, encompassing different established ``patterns of justice'' that correspond to different normative positions. We show that the most popular group fairness metrics can be interpreted as special cases of our approach. Thus, we provide a unifying and interpretative framework for group fairness metrics that reveals the normative choices associated with each of them and that allows understanding their moral substance. At the same time, we provide an extension of the space of possible fairness metrics beyond the ones currently discussed in the fair ML literature. Our framework also allows overcoming several limitations of group fairness metrics that have been criticized in the literature, most notably (1) that they are parity-based, i.e., that they demand some form of equality between groups, which may sometimes be harmful to marginalized groups, (2) that they only compare decisions across groups, but not the resulting consequences for these groups, and (3) that the full breadth of the distributive justice literature is not sufficiently represented.},
  poster = {distributive_justice_poster.pdf}
}

@article{https://doi.org/10.48550/arxiv.2206.02891,
  title = {A Justice-Based Framework for the Analysis of Algorithmic Fairness-Utility Trade-Offs},
  author = {Hertweck, Corinna and Baumann, Joachim and Loi, Michele and Vigan√≤, Eleonora and Heitz, Christoph},
  journal = {arXiv},
  doi = {10.48550/ARXIV.2206.02891},
  year = {2022},
  arxiv = {2206.02891},
  abstract = {In prediction-based decision-making systems, different perspectives can be at odds: The short-term business goals of the decision makers are often in conflict with the decision subjects' wish to be treated fairly. Balancing these two perspectives is a question of values. We provide a framework to make these value-laden choices clearly visible. For this, we assume that we are given a trained model and want to find decision rules that balance the perspective of the decision maker and of the decision subjects. We provide an approach to formalize both perspectives, i.e., to assess the utility of the decision maker and the fairness towards the decision subjects. In both cases, the idea is to elicit values from decision makers and decision subjects that are then turned into something measurable. For the fairness evaluation, we build on the literature on welfare-based fairness and ask what a fair distribution of utility (or welfare) looks like. In this step, we build on well-known theories of distributive justice. This allows us to derive a fairness score that we then compare to the decision maker's utility for many different decision rules. This way, we provide an approach for balancing the utility of the decision maker and the fairness towards the decision subjects for a prediction-based decision-making system.}
}